#!/usr/bin/env python3
"""
AI-Powered Predictive Maintenance Engine
Analyzes system metrics and predicts maintenance needs using machine learning
"""

import os
import sys
import json
import sqlite3
import time
import logging
import statistics
import threading
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
import math
import collections

# ML imports with fallbacks
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False

try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False

# System monitoring
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

@dataclass
class MaintenanceAlert:
    """Maintenance alert generated by AI analysis"""
    alert_id: str
    server_id: int
    alert_type: str
    severity: str  # critical, warning, info
    component: str
    prediction: str
    confidence: float
    recommended_action: str
    time_to_failure: Optional[int]  # hours
    created_at: datetime
    
@dataclass
class SystemMetric:
    """System metric data point"""
    server_id: int
    metric_type: str
    value: float
    unit: str
    timestamp: datetime

class AIMaintenanceEngine:
    """AI-powered predictive maintenance engine"""
    
    def __init__(self, db_path: Path = None):
        self.db_path = db_path or Path.home() / ".unified-system-manager" / "ai-maintenance.db"
        self.base_path = self.db_path.parent
        self.base_path.mkdir(exist_ok=True)
        
        # Initialize logging
        self.setup_logging()
        
        # Initialize database
        self.init_database()
        
        # Analysis parameters
        self.analysis_window = 168  # hours (1 week)
        self.prediction_horizon = 72  # hours (3 days)
        self.confidence_threshold = 0.7
        
        # Metric thresholds and patterns
        self.thresholds = {
            'cpu_percent': {'warning': 80, 'critical': 95},
            'memory_percent': {'warning': 85, 'critical': 95},
            'disk_percent': {'warning': 80, 'critical': 90},
            'disk_io_wait': {'warning': 10, 'critical': 20},
            'network_errors': {'warning': 10, 'critical': 50},
            'temperature': {'warning': 70, 'critical': 80},
            'load_average': {'warning': 2.0, 'critical': 4.0}
        }
        
        # Running state
        self.running = False
        self.analysis_thread = None
        self.last_analysis = {}
        
        self.logger.info("AI Maintenance Engine initialized")
    
    def setup_logging(self):
        """Setup logging configuration"""
        log_path = self.base_path / "logs"
        log_path.mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_path / "ai-maintenance.log"),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(f"{__name__}.AIMaintenanceEngine")
    
    def init_database(self):
        """Initialize database for AI maintenance"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # Metric history for analysis
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS metric_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    server_id INTEGER NOT NULL,
                    metric_type TEXT NOT NULL,
                    value REAL NOT NULL,
                    unit TEXT,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    INDEX(server_id, metric_type, timestamp)
                )
            """)
            
            # AI predictions and alerts
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS ai_predictions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    server_id INTEGER NOT NULL,
                    prediction_type TEXT NOT NULL,
                    component TEXT NOT NULL,
                    predicted_value REAL,
                    confidence REAL NOT NULL,
                    time_horizon INTEGER,
                    model_version TEXT,
                    features_used TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # Maintenance alerts
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS maintenance_alerts (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    alert_id TEXT UNIQUE NOT NULL,
                    server_id INTEGER NOT NULL,
                    alert_type TEXT NOT NULL,
                    severity TEXT NOT NULL,
                    component TEXT NOT NULL,
                    prediction TEXT NOT NULL,
                    confidence REAL NOT NULL,
                    recommended_action TEXT,
                    time_to_failure INTEGER,
                    acknowledged BOOLEAN DEFAULT FALSE,
                    resolved BOOLEAN DEFAULT FALSE,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    acknowledged_at TIMESTAMP,
                    resolved_at TIMESTAMP
                )
            """)
            
            # Pattern analysis results
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS pattern_analysis (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    server_id INTEGER NOT NULL,
                    pattern_type TEXT NOT NULL,
                    pattern_data TEXT NOT NULL,
                    detected_anomalies TEXT,
                    confidence REAL,
                    analysis_window_hours INTEGER,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # Model performance tracking
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS model_performance (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    model_name TEXT NOT NULL,
                    prediction_type TEXT NOT NULL,
                    accuracy REAL,
                    precision_score REAL,
                    recall_score REAL,
                    false_positive_rate REAL,
                    evaluation_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    sample_size INTEGER
                )
            """)
            
            conn.commit()
    
    def collect_system_metrics(self, server_id: int) -> Dict[str, float]:
        """Collect comprehensive system metrics"""
        metrics = {}
        
        if PSUTIL_AVAILABLE:
            try:
                # CPU metrics
                cpu_percent = psutil.cpu_percent(interval=1)
                cpu_count = psutil.cpu_count()
                load_avg = os.getloadavg() if hasattr(os, 'getloadavg') else (0, 0, 0)
                
                metrics.update({
                    'cpu_percent': cpu_percent,
                    'cpu_count': cpu_count,
                    'load_average_1min': load_avg[0],
                    'load_average_5min': load_avg[1],
                    'load_average_15min': load_avg[2]
                })
                
                # Memory metrics
                memory = psutil.virtual_memory()
                swap = psutil.swap_memory()
                
                metrics.update({
                    'memory_percent': memory.percent,
                    'memory_available_gb': memory.available / (1024**3),
                    'memory_used_gb': memory.used / (1024**3),
                    'swap_percent': swap.percent
                })
                
                # Disk metrics
                disk_usage = psutil.disk_usage('/')
                disk_io = psutil.disk_io_counters()
                
                metrics.update({
                    'disk_percent': (disk_usage.used / disk_usage.total) * 100,
                    'disk_free_gb': disk_usage.free / (1024**3),
                    'disk_used_gb': disk_usage.used / (1024**3)
                })
                
                if disk_io:
                    metrics.update({
                        'disk_read_bytes': disk_io.read_bytes,
                        'disk_write_bytes': disk_io.write_bytes,
                        'disk_read_time': disk_io.read_time,
                        'disk_write_time': disk_io.write_time
                    })
                
                # Network metrics
                network_io = psutil.net_io_counters()
                if network_io:
                    metrics.update({
                        'network_bytes_sent': network_io.bytes_sent,
                        'network_bytes_recv': network_io.bytes_recv,
                        'network_packets_sent': network_io.packets_sent,
                        'network_packets_recv': network_io.packets_recv,
                        'network_errors_in': network_io.errin,
                        'network_errors_out': network_io.errout,
                        'network_drops_in': network_io.dropin,
                        'network_drops_out': network_io.dropout
                    })
                
                # Process metrics
                process_count = len(psutil.pids())
                boot_time = psutil.boot_time()
                uptime_hours = (time.time() - boot_time) / 3600
                
                metrics.update({
                    'process_count': process_count,
                    'uptime_hours': uptime_hours
                })
                
                # Temperature (if available)
                try:
                    temps = psutil.sensors_temperatures()
                    if temps:
                        avg_temp = statistics.mean([
                            temp.current for sensor_temps in temps.values() 
                            for temp in sensor_temps if temp.current
                        ])
                        metrics['temperature_avg'] = avg_temp
                except:
                    pass
                
            except Exception as e:
                self.logger.error(f"Error collecting metrics: {e}")
        
        return metrics
    
    def store_metrics(self, server_id: int, metrics: Dict[str, float]):
        """Store metrics in database"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            timestamp = datetime.now()
            for metric_type, value in metrics.items():
                cursor.execute("""
                    INSERT INTO metric_history (server_id, metric_type, value, timestamp)
                    VALUES (?, ?, ?, ?)
                """, (server_id, metric_type, value, timestamp))
            
            conn.commit()
    
    def get_metric_history(self, server_id: int, metric_type: str, hours: int = 168) -> List[Tuple[datetime, float]]:
        """Get metric history for analysis"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            cutoff_time = datetime.now() - timedelta(hours=hours)
            cursor.execute("""
                SELECT timestamp, value
                FROM metric_history
                WHERE server_id = ? AND metric_type = ? AND timestamp > ?
                ORDER BY timestamp
            """, (server_id, metric_type, cutoff_time))
            
            return [(datetime.fromisoformat(row[0]), row[1]) for row in cursor.fetchall()]
    
    def detect_anomalies(self, data: List[Tuple[datetime, float]], threshold_factor: float = 2.0) -> List[Dict[str, Any]]:
        """Detect anomalies in metric data using statistical methods"""
        if len(data) < 10:
            return []
        
        values = [point[1] for point in data]
        
        # Calculate statistical measures
        mean = statistics.mean(values)
        try:
            stdev = statistics.stdev(values)
        except statistics.StatisticsError:
            return []
        
        # Detect outliers using standard deviation
        anomalies = []
        for timestamp, value in data:
            z_score = abs(value - mean) / stdev if stdev > 0 else 0
            
            if z_score > threshold_factor:
                anomalies.append({
                    'timestamp': timestamp,
                    'value': value,
                    'z_score': z_score,
                    'mean': mean,
                    'stdev': stdev,
                    'deviation': value - mean
                })
        
        return anomalies
    
    def predict_trend(self, data: List[Tuple[datetime, float]], hours_ahead: int = 24) -> Dict[str, Any]:
        """Predict future trend using simple linear regression"""
        if len(data) < 5:
            return {'prediction': None, 'confidence': 0, 'trend': 'insufficient_data'}
        
        # Convert to numerical format for analysis
        timestamps = [(point[0] - data[0][0]).total_seconds() / 3600 for point in data]  # hours since start
        values = [point[1] for point in data]
        
        # Simple linear regression
        n = len(data)
        sum_x = sum(timestamps)
        sum_y = sum(values)
        sum_xy = sum(x * y for x, y in zip(timestamps, values))
        sum_x2 = sum(x * x for x in timestamps)
        
        # Calculate slope and intercept
        if n * sum_x2 - sum_x * sum_x == 0:
            return {'prediction': None, 'confidence': 0, 'trend': 'no_trend'}
        
        slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)
        intercept = (sum_y - slope * sum_x) / n
        
        # Predict future value
        future_x = timestamps[-1] + hours_ahead
        predicted_value = slope * future_x + intercept
        
        # Calculate confidence based on correlation
        mean_x = sum_x / n
        mean_y = sum_y / n
        
        numerator = sum((x - mean_x) * (y - mean_y) for x, y in zip(timestamps, values))
        denom_x = sum((x - mean_x) ** 2 for x in timestamps)
        denom_y = sum((y - mean_y) ** 2 for y in values)
        
        correlation = numerator / math.sqrt(denom_x * denom_y) if denom_x * denom_y > 0 else 0
        confidence = abs(correlation)
        
        # Determine trend
        if abs(slope) < 0.01:
            trend = 'stable'
        elif slope > 0:
            trend = 'increasing'
        else:
            trend = 'decreasing'
        
        return {
            'prediction': predicted_value,
            'confidence': confidence,
            'trend': trend,
            'slope': slope,
            'current_value': values[-1],
            'change_rate': slope
        }
    
    def analyze_server(self, server_id: int) -> List[MaintenanceAlert]:
        """Perform comprehensive AI analysis on a server"""
        alerts = []
        
        # Get metric types to analyze
        metric_types = [
            'cpu_percent', 'memory_percent', 'disk_percent', 
            'load_average_1min', 'network_errors_in', 'temperature_avg'
        ]
        
        for metric_type in metric_types:
            try:
                # Get historical data
                history = self.get_metric_history(server_id, metric_type, self.analysis_window)
                if len(history) < 10:
                    continue
                
                # Detect anomalies
                anomalies = self.detect_anomalies(history)
                
                # Predict trend
                prediction = self.predict_trend(history, self.prediction_horizon)
                
                # Generate alerts based on analysis
                alerts.extend(self._generate_alerts(
                    server_id, metric_type, history, anomalies, prediction
                ))
                
            except Exception as e:
                self.logger.error(f"Error analyzing {metric_type} for server {server_id}: {e}")
        
        # Store alerts
        for alert in alerts:
            self._store_alert(alert)
        
        return alerts
    
    def _generate_alerts(self, server_id: int, metric_type: str, history: List[Tuple[datetime, float]], 
                        anomalies: List[Dict], prediction: Dict[str, Any]) -> List[MaintenanceAlert]:
        """Generate maintenance alerts based on analysis"""
        alerts = []
        current_value = history[-1][1] if history else 0
        
        # Check for threshold violations
        if metric_type in self.thresholds:
            thresholds = self.thresholds[metric_type]
            
            if current_value >= thresholds['critical']:
                alerts.append(MaintenanceAlert(
                    alert_id=f"crit_{server_id}_{metric_type}_{int(time.time())}",
                    server_id=server_id,
                    alert_type="threshold_critical",
                    severity="critical",
                    component=metric_type,
                    prediction=f"{metric_type} is critically high: {current_value:.1f}",
                    confidence=0.95,
                    recommended_action=self._get_recommended_action(metric_type, "critical"),
                    time_to_failure=None,
                    created_at=datetime.now()
                ))
            elif current_value >= thresholds['warning']:
                alerts.append(MaintenanceAlert(
                    alert_id=f"warn_{server_id}_{metric_type}_{int(time.time())}",
                    server_id=server_id,
                    alert_type="threshold_warning",
                    severity="warning",
                    component=metric_type,
                    prediction=f"{metric_type} is elevated: {current_value:.1f}",
                    confidence=0.85,
                    recommended_action=self._get_recommended_action(metric_type, "warning"),
                    time_to_failure=None,
                    created_at=datetime.now()
                ))
        
        # Check for anomalies
        if anomalies and len(anomalies) > 0:
            recent_anomalies = [a for a in anomalies if 
                             (datetime.now() - a['timestamp']).total_seconds() < 3600]  # Last hour
            
            if recent_anomalies:
                avg_z_score = statistics.mean([a['z_score'] for a in recent_anomalies])
                alerts.append(MaintenanceAlert(
                    alert_id=f"anom_{server_id}_{metric_type}_{int(time.time())}",
                    server_id=server_id,
                    alert_type="anomaly_detected",
                    severity="warning" if avg_z_score < 3 else "critical",
                    component=metric_type,
                    prediction=f"Unusual {metric_type} pattern detected (Z-score: {avg_z_score:.1f})",
                    confidence=min(0.9, avg_z_score / 3),
                    recommended_action="Investigate recent system changes and resource usage",
                    time_to_failure=None,
                    created_at=datetime.now()
                ))
        
        # Check for concerning trends
        if prediction.get('confidence', 0) > self.confidence_threshold:
            pred_value = prediction.get('prediction', 0)
            trend = prediction.get('trend', 'unknown')
            
            if metric_type in self.thresholds and pred_value:
                time_to_critical = None
                
                if trend == 'increasing' and pred_value > self.thresholds[metric_type]['critical']:
                    # Calculate time to failure
                    change_rate = prediction.get('change_rate', 0)
                    if change_rate > 0:
                        current_val = prediction.get('current_value', 0)
                        critical_threshold = self.thresholds[metric_type]['critical']
                        time_to_critical = int((critical_threshold - current_val) / change_rate)
                    
                    alerts.append(MaintenanceAlert(
                        alert_id=f"trend_{server_id}_{metric_type}_{int(time.time())}",
                        server_id=server_id,
                        alert_type="trend_prediction",
                        severity="warning",
                        component=metric_type,
                        prediction=f"{metric_type} trending toward critical levels: {pred_value:.1f}",
                        confidence=prediction['confidence'],
                        recommended_action=self._get_recommended_action(metric_type, "preventive"),
                        time_to_failure=time_to_critical,
                        created_at=datetime.now()
                    ))
        
        return alerts
    
    def _get_recommended_action(self, metric_type: str, severity: str) -> str:
        """Get recommended action for metric and severity"""
        actions = {
            'cpu_percent': {
                'critical': 'Identify and terminate high-CPU processes immediately',
                'warning': 'Monitor CPU-intensive processes and consider load balancing',
                'preventive': 'Review process scheduling and consider CPU upgrade'
            },
            'memory_percent': {
                'critical': 'Free memory immediately by stopping non-essential services',
                'warning': 'Monitor memory usage and identify memory leaks',
                'preventive': 'Plan memory upgrade or optimize memory usage'
            },
            'disk_percent': {
                'critical': 'Free disk space immediately - remove logs and temp files',
                'warning': 'Clean up unnecessary files and monitor disk growth',
                'preventive': 'Schedule disk expansion or implement log rotation'
            },
            'load_average_1min': {
                'critical': 'Reduce system load immediately - stop non-critical processes',
                'warning': 'Investigate high load sources and optimize',
                'preventive': 'Consider load balancing or hardware upgrade'
            },
            'temperature_avg': {
                'critical': 'Check cooling system immediately - risk of hardware damage',
                'warning': 'Monitor temperature and check ventilation',
                'preventive': 'Service cooling system and clean dust filters'
            }
        }
        
        return actions.get(metric_type, {}).get(severity, 'Manual investigation required')
    
    def _store_alert(self, alert: MaintenanceAlert):
        """Store alert in database"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT OR REPLACE INTO maintenance_alerts (
                    alert_id, server_id, alert_type, severity, component,
                    prediction, confidence, recommended_action, time_to_failure, created_at
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                alert.alert_id, alert.server_id, alert.alert_type, alert.severity,
                alert.component, alert.prediction, alert.confidence,
                alert.recommended_action, alert.time_to_failure, alert.created_at
            ))
            
            conn.commit()
    
    def get_active_alerts(self, server_id: int = None) -> List[Dict[str, Any]]:
        """Get active maintenance alerts"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            if server_id:
                cursor.execute("""
                    SELECT alert_id, server_id, alert_type, severity, component,
                           prediction, confidence, recommended_action, time_to_failure, created_at
                    FROM maintenance_alerts
                    WHERE server_id = ? AND resolved = FALSE
                    ORDER BY severity DESC, created_at DESC
                """, (server_id,))
            else:
                cursor.execute("""
                    SELECT alert_id, server_id, alert_type, severity, component,
                           prediction, confidence, recommended_action, time_to_failure, created_at
                    FROM maintenance_alerts
                    WHERE resolved = FALSE
                    ORDER BY severity DESC, created_at DESC
                """)
            
            alerts = []
            for row in cursor.fetchall():
                alerts.append({
                    'alert_id': row[0],
                    'server_id': row[1],
                    'alert_type': row[2],
                    'severity': row[3],
                    'component': row[4],
                    'prediction': row[5],
                    'confidence': row[6],
                    'recommended_action': row[7],
                    'time_to_failure': row[8],
                    'created_at': row[9]
                })
            
            return alerts
    
    def start_analysis(self, analysis_interval: int = 300):  # 5 minutes
        """Start continuous AI analysis"""
        if self.running:
            return
        
        self.running = True
        self.analysis_thread = threading.Thread(
            target=self._analysis_loop,
            args=(analysis_interval,),
            daemon=True,
            name="AIAnalysisThread"
        )
        self.analysis_thread.start()
        
        self.logger.info("Started AI maintenance analysis")
    
    def stop_analysis(self):
        """Stop AI analysis"""
        self.running = False
        if self.analysis_thread:
            self.analysis_thread.join(timeout=10)
        
        self.logger.info("Stopped AI maintenance analysis")
    
    def _analysis_loop(self, interval: int):
        """Main analysis loop"""
        while self.running:
            try:
                # For demo, analyze localhost (server_id = 1)
                server_id = 1
                
                # Collect current metrics
                metrics = self.collect_system_metrics(server_id)
                if metrics:
                    self.store_metrics(server_id, metrics)
                
                # Perform AI analysis
                alerts = self.analyze_server(server_id)
                
                if alerts:
                    self.logger.info(f"Generated {len(alerts)} maintenance alerts for server {server_id}")
                    for alert in alerts:
                        self.logger.warning(f"ALERT: {alert.severity.upper()} - {alert.prediction}")
                
                # Update last analysis time
                self.last_analysis[server_id] = datetime.now()
                
                time.sleep(interval)
                
            except Exception as e:
                self.logger.error(f"Analysis loop error: {e}")
                time.sleep(30)  # Wait before retrying
    
    def get_analysis_summary(self) -> Dict[str, Any]:
        """Get comprehensive analysis summary"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # Alert statistics
            cursor.execute("SELECT COUNT(*) FROM maintenance_alerts WHERE resolved = FALSE")
            active_alerts = cursor.fetchone()[0]
            
            cursor.execute("SELECT severity, COUNT(*) FROM maintenance_alerts WHERE resolved = FALSE GROUP BY severity")
            alert_by_severity = dict(cursor.fetchall())
            
            # Recent metrics count
            cutoff_time = datetime.now() - timedelta(hours=24)
            cursor.execute("SELECT COUNT(*) FROM metric_history WHERE timestamp > ?", (cutoff_time,))
            recent_metrics = cursor.fetchone()[0]
            
            # Analysis status
            analysis_status = {
                'running': self.running,
                'last_analysis': self.last_analysis,
                'analysis_window_hours': self.analysis_window,
                'prediction_horizon_hours': self.prediction_horizon
            }
            
            return {
                'alerts': {
                    'total_active': active_alerts,
                    'by_severity': alert_by_severity
                },
                'metrics': {
                    'recent_count': recent_metrics,
                    'collection_rate': recent_metrics / 24 if recent_metrics > 0 else 0
                },
                'analysis': analysis_status,
                'timestamp': datetime.now().isoformat()
            }

def main():
    """Main function for testing"""
    print("ü§ñ AI Maintenance Engine - Testing Mode")
    
    engine = AIMaintenanceEngine()
    
    # Start analysis
    engine.start_analysis(60)  # Analyze every minute for testing
    
    try:
        print("üîÑ Running AI analysis for 5 minutes...")
        time.sleep(300)  # Run for 5 minutes
        
        # Get summary
        summary = engine.get_analysis_summary()
        print(f"üìä Analysis Summary: {json.dumps(summary, indent=2)}")
        
        # Get active alerts
        alerts = engine.get_active_alerts()
        print(f"üö® Active Alerts: {len(alerts)}")
        for alert in alerts[:5]:  # Show first 5
            print(f"  - {alert['severity'].upper()}: {alert['prediction']}")
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Stopping analysis...")
    finally:
        engine.stop_analysis()

if __name__ == "__main__":
    main()